{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Slimanee/cours-insa/blob/master/datascientest_itineraire.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ETAPE 1 \n",
        "\n",
        "## 1- Récupération des données : \n",
        "\n",
        "**Source :** https://diffuseur.datatourisme.fr/fr/\n",
        "\n",
        "> Nous avons tester la récupération d'un fichier unique JSON structuré généré via le site de Data Tourisme et d'un fichier CSV déjà créé sur data.gouv. <br>\n",
        "A priori le JSON est de meilleur qualité. \n",
        "\n",
        "Pour ce projet nous choisissons la **région Auvergne-Rhones-Alpes**.\n",
        "<br>\n",
        "\n",
        "> Nous avons testé plusieurs moyens de récupération de fichiers.\n",
        "\n",
        "> Le site https://diffuseur.datatourisme.fr/fr/ permet d'automatiser les flux, de choisir le format de sortie ainsi que le périmètre que nous voulons analyser\n",
        "\n",
        "> Tests effectués avec les formats suivants :\n",
        "- Fichiers json structurés\n",
        "- Fichiers csv (liens http sur lesquels il faudrait faire du webscrapping pour récupérer les data des POI et un travail sur les longitudes/latitudes)\n",
        "- Archive zip / fichier json (contient un ensemble de sous dossiers avec un json \"entete\" permettant d'obtenir le chemin et les data POI.)\n",
        "\n",
        "> Les données datatourism sont aussi déposées de manière régulière sur le site https://www.data.gouv.fr/ . \n",
        "- fichier csv mis à jour quotidiennement\n",
        "\n",
        "Pour ce projet nous avons fait le choix d'automatiser un flux sur le site https://diffuseur.datatourisme.fr/fr/ au format json qui contient les données nécessaires et nous avons choisi la **région Auvergne-Rhones-Alpes**. \n",
        "<br> \n",
        "\n",
        "## 2- Trie et nettoyage des données\n",
        "\n",
        "> Nous avons effectué un premier nettoyage rapide, simple tout en gardant un maximum d'informations pour cette étape. <br>\n",
        "Nous allons ensuite travailler sur la colonnne \"type\" des POI, qui nous permettra de relier les POI entre eux et de créer un filtre sur l'application. "
      ],
      "metadata": {
        "id": "i684Ia9WRYo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import seaborn as sns\n",
        "from sqlalchemy import create_engine"
      ],
      "metadata": {
        "id": "8htsQjVRV8QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1ioCvDgUHHz"
      },
      "outputs": [],
      "source": [
        "key = '380d2fe9-2c9c-4190-a79e-8301b37d03fb'\n",
        "url = 'https://diffuseur.datatourisme.fr/webservice/bfadcf44012b7156ca3e297b468c4f75/' + key\n",
        "data = requests.get(url).json()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.json_normalize(data['@graph'])\n",
        "columns = {\n",
        "    'dc:identifier': 'id',\n",
        "    '@id': 'url',\n",
        "    '@type': 'type',\n",
        "    'rdfs:label.@value': 'nom',\n",
        "    'rdfs:comment.@value': 'commentaire',\n",
        "    'hasContact.schema:email': 'contact_email',\n",
        "    'hasContact.schema:telephone': 'contact_telephone',\n",
        "    'hasContact.foaf:homepage': 'contact_homepage',\n",
        "    'isLocatedAt.schema:address.schema:streetAddress': 'adresse',\n",
        "    'isLocatedAt.schema:address.schema:addressLocality': 'ville',\n",
        "    'isLocatedAt.schema:address.schema:postalCode': 'code_postal',\n",
        "    'isLocatedAt.schema:geo.schema:latitude.@value': 'latitude',\n",
        "    'isLocatedAt.schema:geo.schema:longitude.@value': 'longitude',\n",
        "    'isLocatedAt.schema:geo.latlon.@value': 'latlon',\n",
        "    'isLocatedAt.schema:geo.schema:elevation.@value': 'altitude',\n",
        "    'isLocatedAt.schema:openingHoursSpecification.schema:validFrom.@value': 'ouvert_date_de',\n",
        "    'isLocatedAt.schema:openingHoursSpecification.schema:validThrough.@value': 'ouvert_date_a',\n",
        "    'isLocatedAt.schema:openingHoursSpecification.schema:opens.@value': 'ouvert_heure_de',\n",
        "    'isLocatedAt.schema:openingHoursSpecification.schema:closes.@value': 'ouvert_heure_a',\n",
        "    'isLocatedAt.schema:openingHoursSpecification.additionalInformation.@value': 'horaire',\n",
        "    'schema:offers.schema:priceSpecification.schema:minPrice': 'prix_min',\n",
        "    'schema:offers.schema:priceSpecification.schema:maxPrice': 'prix_max',\n",
        "    'schema:offers.schema:priceSpecification.schema:priceCurrency': 'currency',\n",
        "    'schema:offers.schema:priceSpecification.appliesOnPeriod.startDate.@value': 'prix_de',\n",
        "    'schema:offers.schema:priceSpecification.appliesOnPeriod.endDate.@value': 'prix_a',\n",
        "}\n",
        "df = df[columns.keys()]  # Keep only useful columns\n",
        "df = df.rename(columns=columns)\n",
        "df = df.dropna(subset=['id', 'nom', 'longitude', 'latitude', 'latlon'])  # Suppress row without mandatory data\n",
        "df = df.set_index('id', verify_integrity=True)  # Ensure column id contains only unique values\n",
        "df.insert(len(df.columns), 'updated_at', pd.Timestamp.utcnow())  # Add datetime column to know when data were refreshed\n",
        "\n",
        "# Filter/cleanup useful types\n",
        "df['type'] = df['type'].apply(lambda x: list({i.replace('schema:', '') for i in x} - {'urn:resource', 'olo:OrderedList', 'PlaceOfInterest', 'PointOfInterest'}))\n",
        "df_types = df.explode(column='type')[['type', 'updated_at']]  # Create type dataframe for types mapping\n",
        "df_types.index.names = ['poi_id']  # Change id to poi_id in type dataframe\n",
        "df = df.drop(columns=['type'])  # Remove type column from poi dataframe\n",
        "\n",
        "# Extract price min/max info from list/dictionary if exists\n",
        "df['prix_min'] = df['prix_min'].apply(lambda x: [e['@value'] for e in x] if isinstance(x, list) else x)\n",
        "df['prix_max'] = df['prix_max'].apply(lambda x: [e['@value'] for e in x] if isinstance(x, list) else x)\n",
        "\n",
        "df = df.applymap(lambda x: ', '.join(x) if isinstance(x, list) else x)  # Transform lists into comma-separated strings"
      ],
      "metadata": {
        "id": "-630r-SAsU9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_types.head())\n",
        "pd.set_option('display.max_columns', 30)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "9B9rRGTOxdd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualisation rapide de notre dataframe POI. Le noir correspond au données remplies, le beige représente les NaN.\n",
        "# Les principales informations des POI nécessaires au projet sont présentes : \"nom\", \"longitude\", \"latitude\", \"latlon\".\n",
        "sns.heatmap(df.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "ZX0n9sd2WPUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ETAPE 2\n",
        "\n",
        "## 1- Création d'une database relationnelle (SQL)\n",
        "(en cours)\n"
      ],
      "metadata": {
        "id": "bXRW-c8OLOom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo apt-get -y -qq update\n",
        "!sudo apt-get -y -qq install postgresql\n",
        "!sudo service postgresql start\n",
        "!sudo -u postgres psql -U postgres -c \"ALTER USER postgres PASSWORD 'postgres';\"\n",
        "%env DATABASE_URL=postgresql://postgres:postgres@localhost:5432/postgres\n",
        "%load_ext sql"
      ],
      "metadata": {
        "id": "UwZZYOP5sKCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creation de deux tables postgres pour les POI et les types\n",
        "engine = create_engine(os.environ['DATABASE_URL'])\n",
        "with engine.begin() as connection:\n",
        "    df.to_sql('itineraire_poi', connection, if_exists='replace')\n",
        "    df_types.to_sql('itineraire_types', connection, if_exists='replace')"
      ],
      "metadata": {
        "id": "XsZWJeR8Ek_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "select * from itineraire_poi limit 5;"
      ],
      "metadata": {
        "id": "joxQn_14uYUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "select * from itineraire_types limit 5;"
      ],
      "metadata": {
        "id": "ZSg9vQYVsqqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Idées\n",
        "\n",
        "* Creation de catégories: https://towardsdatascience.com/fuzzy-string-matching-in-python-68f240d910fe\n",
        "* idée 2\n",
        "\n"
      ],
      "metadata": {
        "id": "htOXw-brV456"
      }
    }
  ]
}